{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g88HgXnuUtV1"
   },
   "source": [
    "# Duboko učenje - laboratorijska vježba - generativni modeli - Generative adversarial networks (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMiK8Z1xUtV2"
   },
   "source": [
    "Primarna namjena GAN-a je isto generiranje novih i uvjerljivih uzoraka, no princip rada je malo drugačiji od prethodna dva modela. GAN ne procjenjuje direktno parametre $p(\\mathbf x)$ ili bilo koje druge distribucije, premda se njegovo treniranje može interpretirati kao estimacija $p(\\mathbf x)$. Najvjerojatnije zahvaljujući tom drugačijem pristupu, GAN-ovi često generiraju vizualno najbolje uzorke u usporedbi sa VAE ili drugim generativnim mrežama.\n",
    "\n",
    "GAN se sastoji od dvije zasebne mreže \n",
    "\n",
    "1. Generator (G) koji ima zadatak generirati uvjerljive uzorke\n",
    "2. Diskriminator (D) koji ima zadatak prepoznati radi li se o pravom uzorku (iz skupa za treniranje) ili lažnom uzorku koji je generirao G\n",
    "\n",
    "<center>\n",
    "<div class=\"fig figcenter fighighlight\">\n",
    "  <img src=\"https://dlunizg.github.io/assets/lab4/GAN.svg\" width=\"30%\">\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Te dvije mreže su protivnici (Adversaries), imaju dijametralno suprotstavljene ciljeve te se pokušavaju nadmudriti. To nadmetanje ih tjera da budu sve bolji u postizanju svog cilja i da se fokusiraju na sve bitne detalje ulaznih podataka. Očekivano, njihovo nadmetanje trebalo bi dovesti do toga da generator generira savršene uzorke koje diskriminator ne može razlikovati od uzoraka iz skupa za treniranje. Da bi generator postigao takav uspjeh nužno je da i diskriminator bude maksimalno dobar u svom zadatku.\n",
    "\n",
    "Generator na svojem izlazu generira uzorke za neki slučajni ulazni vektor koji prati neku distribuciju. Ta slučajnost na ulazu omogućuje generatoru da uvijek generira nove uzorke. Pri tome nema nekih posebnih ograničenja na arhitekturu generatora, no poželjno je da se može trenirati backpropagation algoritmom. \n",
    "\n",
    "<center>\n",
    "<div class=\"fig figcenter fighighlight\">\n",
    "  <img src=\"https://dlunizg.github.io/assets/lab4/G.svg\" width=\"30%\">\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Diskriminator na svome izlazu treba estimirati pripadnost razredu stvarnih ili lažnih uzoraka za svaki ulazni vektor. Za razliku od generatora, ovdje je moguće koristiti učenje pod nadzorom jer se za svaki uzorak zna da li je došao iz skupa za treniranje ili od generatora. Radi jednostavnosti možemo izlaz diskriminatora ograničiti u rasponu $[0,1]$ i interpretirati kao vjerojatnost da je ulazni uzorak stvaran (iz skupa za treniranje).\n",
    "\n",
    "    \n",
    "<center>\n",
    "<div class=\"fig figcenter fighighlight\">\n",
    "  <img src=\"https://dlunizg.github.io/assets/lab4/D.svg\" width=\"30%\">\n",
    "</div>\n",
    "</center>\n",
    "    \n",
    "\n",
    "Gore opisani ciljevi diskriminatora i generatora mogu se formalno izraziti u sljedećoj funkciji cilja:\n",
    "\n",
    "$\\min_G \\max_D V(D,G) = E_{ \\mathbf x \\sim p_{data}(\\mathbf x) } [\\log D( \\mathbf x)] + E_{ \\mathbf z  \\sim p_{\\mathbf z}(\\mathbf z) } [\\log(1 - D(G( \\mathbf z)))]$\n",
    "\n",
    "Prvi pribrojnik predstavlja očekivanje procjene log vjerojatnosti da su uzorci iz skupa za treniranje stvarni. Drugi pribrojnik predstavlja očekivanje procjene log vjerojatnosti da generirani uzorci nisu stvarni, tj. da su umjetni. Diskriminator ima za cilj maksimizirati oba pribrojnika, dok generator ima za cilj minimizirati drugi pribrojnik. Svaki pribrojnik funkcije cilja može se jednostavno procijeniti za jednu mini grupu te se može procijeniti gradijent s obzirom na prametre obiju mreža. \n",
    "\n",
    "Treniranje dviju mreža (G i D) može se provesti istovremeno ili se u jednoj iteraciji prvo može trenirati jedna mreža a zatim druga. Dodatno, neki autori preporučuju da se u nekoliko uzastopnih iteracija trenira jedna mreža, a nakon toga druga mreža samo jednu iteraciju.\n",
    "\n",
    "    \n",
    "<center>\n",
    "<div class=\"fig figcenter fighighlight\">\n",
    "  <img src=\"https://dlunizg.github.io/assets/lab4/GAN2.svg\" width=\"50%\">\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "\n",
    "Kod generiranja slika uspješnim se pokazao Deep Convolutional GAN (DCGAN) koji u skrivenim slojevima obiju mreža koristi konvolucijske slojeve. Za razliku od klasičnih konvolucijskih mreža, ovdje se ne koriste pooling slojevi nego se uzorkovanje provodi pomoću konvolucijskih slojeva koji imaju posmak veći od 1. Autori mreže preporučuju korištenje Batch normalizacije u svim slojevima osim u izlaznom sloju generatora te ulaznom i izlaznom sloju diskriminatora. Korištenje Leaky ReLU aktivacijskih funkcija u svim slojevima osim u izlaznim je još jedna specifičnost DCGAN-a kao i eliminacija potpuno povezanih slojeva.\n",
    "\n",
    "    \n",
    "<center>\n",
    "<div class=\"fig figcenter fighighlight\">\n",
    "  <img src=\"https://dlunizg.github.io/assets/lab4/DCGAN.svg\" width=\"50%\">\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "### U sljedećih nekoliko blokova koda nalaze se inicijalizacijske postavke i gotove pomoćne funkcije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6of1WViKUtV6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import torch.distributions as tdist\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vI3Eo32_UtV9"
   },
   "outputs": [],
   "source": [
    "def prepare_data_loaders(batch_size=32):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.Resize(64),\n",
    "                                    torchvision.transforms.ToTensor()\n",
    "                               ])), batch_size=batch_size)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST('./files', train=False, download=True,\n",
    "                                   transform=torchvision.transforms.Compose([\n",
    "                                       torchvision.transforms.Resize(64),\n",
    "                                       torchvision.transforms.ToTensor()\n",
    "                                   ])), batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNj1R-q3UtV9"
   },
   "source": [
    "### Zadatak\n",
    "\n",
    "Implementirajte DCGAN s generatorom i diskriminatorom. Arhitekura treba biti:\n",
    "    \n",
    "* Generator\n",
    "    * Sloj 1 - Broj izlaznih kanala = 512, veličina jezgre = 4, veličina koraka = 1\n",
    "    * Sloj 2 - Broj izlaznih kanala = 256, veličina jezgre = 4, veličina koraka = 2, padding = 1\n",
    "    * Sloj 3 - Broj izlaznih kanala = 128, veličina jezgre = 4, veličina koraka = 2, padding = 1\n",
    "    * Sloj 4 - Broj izlaznih kanala = 64, veličina jezgre = 4, veličina koraka = 2, padding = 1\n",
    "    * Sloj 5 - Broj izlaznih kanala = 1, veličina jezgre = 4, veličina koraka = 2, padding = 1\n",
    "\n",
    "* Diskriminator\n",
    "    * Sloj 1 - Broj izlaznih konvolucija = 64, veličina jezgre = 4, veličina koraka = 2, padding = 1\n",
    "    * Sloj 2 - Broj izlaznih konvolucija = 128, veličina jezgre = 4, veličina koraka = 2, padding = 1\n",
    "    * Sloj 3 - Broj izlaznih konvolucija = 256, veličina jezgre = 4, veličina koraka = 2, padding = 1\n",
    "    * Sloj 4 - Broj izlaznih konvolucija = 512, veličina jezgre = 4, veličina koraka = 2, padding = 1\n",
    "    * Sloj 5 - Broj izlaznih konvolucija = 1, veličina jezgre = 4, veličina koraka = 1, padding = 0\n",
    "\n",
    "Ulaz u generator $\\mathbf z$ neka ima 100 elemenata prema normalnoj distribuciji $N(0,1)$. Ulazni podaci neka su MNIST brojevi skalirani na veličinu 64x64 te treniranje provedite kroz barem 20 epoha. U jednoj iteraciji provedite jednu optimizaciju generatora i jednu optimizaciju diskriminatora s po jednom mini grupom. Koristite tanh aktivacijsku funkciju za izlaz generatora i sigmoid aktivaciju za izlaz diskriminator, a za ostaje slojeve \"propustljivi\" ReLU sa \"negative_slope\" parametrom od 0.2. Batch normalizacija (jedan od podzadataka) ide iza svakog sloja.\n",
    "\n",
    "**Podzadaci:**\n",
    "\n",
    " 1. Vizualizirajte rezultate generiranja 100 novih uzoraka iz slučajnih vektora $\\mathbf z$.\n",
    " 2. Spremite težine istreniranog modela u datoteku \"zad_gan.th\" i uploadajte tu datoteku na Moodle.\n",
    " 3. Na Moodle predajte vizualizaciju 1. podzadatka.\n",
    " 4. Odgovorite na sljedeća pitanja **u bilježnici**. Bilježnicu na kraju predajte na Moodle.\n",
    "\n",
    "\n",
    "Koristite sljedeći predložak:\n",
    "\n",
    "**NAPOMENA**: Osim nadopunjavanja koda koji nedostaje, predložak se treba prilagođavati prema potrebi, a može i prema vlastitim preferencijama. Stoga **budite oprezni s tvrdnjama da vam neki dio koda ne radi!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ih9ryX_hUtV-"
   },
   "source": [
    "**Pitanje:**\n",
    "    \n",
    "U jednoj iteraciji provedite treniranje diskriminatora s dvije minigrupe, a generatora s jednom minigrupom. Ponovite isti postupak samo zamijenite mjesta generatora i diskriminatora. Vizualizirajte generirane uzorke i komentirajte rezultate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvGOY-yJUtV-"
   },
   "source": [
    "**Odgovor:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt7SMrWGUtV_"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHDdurmNUtV_"
   },
   "source": [
    "**Pitanje:**\n",
    "\n",
    "Isključite batch normalizaciju u obje mreže. Komentirajte rezultate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWSSH11TUtV_"
   },
   "source": [
    "**Odgovor:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ddaja2QaUtWA"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dSAHj36eUtWA"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generator koristi transponirane konvolucije (ConvTranspose2d) za povećanje dimenzionalnosti \n",
    "(upsampling) iz latentnog vektora do slike dimenzija 64x64. \n",
    "Batch normalizacija se koristi nakon svakog sloja osim izlaznog.\n",
    "'''\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Sloj 1: Input (N, 100, 1, 1) -> Output (N, 512, 4, 4)\n",
    "        self.ct1 = nn.ConvTranspose2d(latent_size, 512, 4, 1, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Sloj 2: Input (N, 512, 4, 4) -> Output (N, 256, 8, 8)\n",
    "        self.ct2 = nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Sloj 3: Input (N, 256, 8, 8) -> Output (N, 128, 16, 16)\n",
    "        self.ct3 = nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Sloj 4: Input (N, 128, 16, 16) -> Output (N, 64, 32, 32)\n",
    "        self.ct4 = nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Sloj 5: Input (N, 64, 32, 32) -> Output (N, 1, 64, 64)\n",
    "        self.ct5 = nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagacija kroz slojeve: ConvT -> BatchNorm -> Activation\n",
    "        x = self.ct1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.ct2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.ct3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.ct4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.ct5(x)\n",
    "        out = self.tanh(x) # Izlazni sloj koristi Tanh\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generator BEZ Batch Normalizacije\n",
    "class GeneratorNoBN(nn.Module):\n",
    "    def __init__(self, latent_size=100):\n",
    "        super(GeneratorNoBN, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Sloj 1: Input (N, 100, 1, 1) -> Output (N, 512, 4, 4)\n",
    "        self.ct1 = nn.ConvTranspose2d(latent_size, 512, 4, 1, 0, bias=True) # Bias je True jer nema BN\n",
    "        \n",
    "        # Sloj 2: Input (N, 512, 4, 4) -> Output (N, 256, 8, 8)\n",
    "        self.ct2 = nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=True)\n",
    "        \n",
    "        # Sloj 3: Input (N, 256, 8, 8) -> Output (N, 128, 16, 16)\n",
    "        self.ct3 = nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=True)\n",
    "        \n",
    "        # Sloj 4: Input (N, 128, 16, 16) -> Output (N, 64, 32, 32)\n",
    "        self.ct4 = nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=True)\n",
    "        \n",
    "        # Sloj 5: Input (N, 64, 32, 32) -> Output (N, 1, 64, 64)\n",
    "        self.ct5 = nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Nema BN: ConvT -> Activation\n",
    "        x = self.lrelu(self.ct1(x))\n",
    "        x = self.lrelu(self.ct2(x))\n",
    "        x = self.lrelu(self.ct3(x))\n",
    "        x = self.lrelu(self.ct4(x))\n",
    "        \n",
    "        out = self.tanh(self.ct5(x))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qRMWk6CtUtWB"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Diskriminator koristi standardne konvolucije (Conv2d) za smanjenje dimenzionalnosti \n",
    "(downsampling) i klasifikaciju slike kao stvarne ili lažne. \n",
    "Batch normalizacija se ne koristi na ulaznom sloju diskriminatora ni na izlaznom sloju.\n",
    "'''\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Sloj 1: Input (N, 1, 64, 64) -> Output (N, 64, 32, 32)\n",
    "        # Nema Batch Normalizacije na ulazu diskriminatora\n",
    "        self.cv1 = nn.Conv2d(1, 64, 4, 2, 1, bias=False)\n",
    "        \n",
    "        # Sloj 2: Input (N, 64, 32, 32) -> Output (N, 128, 16, 16)\n",
    "        self.cv2 = nn.Conv2d(64, 128, 4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Sloj 3: Input (N, 128, 16, 16) -> Output (N, 256, 8, 8)\n",
    "        self.cv3 = nn.Conv2d(128, 256, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Sloj 4: Input (N, 256, 8, 8) -> Output (N, 512, 4, 4)\n",
    "        self.cv4 = nn.Conv2d(256, 512, 4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Sloj 5: Input (N, 512, 4, 4) -> Output (N, 1, 1, 1)\n",
    "        # Nema Batch Normalizacije na izlazu, koristi se Sigmoid\n",
    "        self.cv5 = nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.cv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.cv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.cv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.cv5(x)\n",
    "        out = self.sigmoid(x)\n",
    "        \n",
    "        return out.squeeze() # Uklanja dimenzije veličine 1 (N, 1, 1, 1) -> (N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Diskriminator BEZ Batch Normalizacije\n",
    "class DiscriminatorNoBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNoBN, self).__init__()\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Sloj 1: (N, 1, 64, 64) -> (N, 64, 32, 32). Prvi sloj nema BN.\n",
    "        self.cv1 = nn.Conv2d(1, 64, 4, 2, 1, bias=True)\n",
    "        \n",
    "        # Sloj 2: (N, 64, 32, 32) -> (N, 128, 16, 16)\n",
    "        self.cv2 = nn.Conv2d(64, 128, 4, 2, 1, bias=True)\n",
    "        \n",
    "        # Sloj 3: (N, 128, 16, 16) -> (N, 256, 8, 8)\n",
    "        self.cv3 = nn.Conv2d(128, 256, 4, 2, 1, bias=True)\n",
    "        \n",
    "        # Sloj 4: (N, 256, 8, 8) -> (N, 512, 4, 4)\n",
    "        self.cv4 = nn.Conv2d(256, 512, 4, 2, 1, bias=True)\n",
    "        \n",
    "        # Sloj 5: (N, 512, 4, 4) -> (N, 1, 1, 1). Izlazni sloj nema BN.\n",
    "        self.cv5 = nn.Conv2d(512, 1, 4, 1, 0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lrelu(self.cv1(x))\n",
    "        x = self.lrelu(self.cv2(x))\n",
    "        x = self.lrelu(self.cv3(x))\n",
    "        x = self.lrelu(self.cv4(x))\n",
    "        \n",
    "        out = self.sigmoid(self.cv5(x))\n",
    "        \n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UUbO_EU-UtWC"
   },
   "outputs": [],
   "source": [
    "def train(gmodel: Generator, dmodel: Discriminator, n_epochs=10, log_epochs=1, batch_size=32, learning_rate=1e-3, device='cpu',\n",
    "          d_updates=1, g_updates=1):\n",
    "    \n",
    "    train_loader, test_loader = prepare_data_loaders(batch_size=batch_size)\n",
    "    \n",
    "    gmodel = gmodel.to(device)\n",
    "    dmodel = dmodel.to(device)\n",
    "    \n",
    "    gmodel.train()\n",
    "    dmodel.train()\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    g_optim = optim.Adam(gmodel.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    d_optim = optim.Adam(dmodel.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    \n",
    "    for epoch_idx in range(0, n_epochs):\n",
    "        \n",
    "        g_loss_accum, d_loss_accum = 0, 0\n",
    "        \n",
    "        for image_data, _ in tqdm.tqdm(train_loader):\n",
    "\n",
    "            current_batch_size = image_data.shape[0] # Stvarna veličina batcha (zadnji batch može biti manji)\n",
    "            image_data = image_data.to(device)\n",
    "\n",
    "            # ------------------------------------\n",
    "            #  1. Train Discriminator (D_updates puta)\n",
    "            # ------------------------------------\n",
    "            for _ in range(d_updates):\n",
    "                dmodel.zero_grad()\n",
    "                \n",
    "                # --- Real data pass ---\n",
    "                labels_real = torch.ones(current_batch_size, device=device).float()\n",
    "                d_output_real = dmodel(image_data)\n",
    "                d_err_real = criterion(d_output_real, labels_real)\n",
    "                d_err_real.backward()\n",
    "\n",
    "                # --- Fake data pass ---\n",
    "                # Novi noise za svaki D update\n",
    "                noise = torch.randn(current_batch_size, 100, 1, 1, device=device) \n",
    "                fake_image_data = gmodel(noise)\n",
    "                \n",
    "                labels_fake = torch.zeros(current_batch_size, device=device).float()\n",
    "                \n",
    "                # .detach() kako ne bismo računali gradijente za generator u ovom koraku\n",
    "                d_output_fake = dmodel(fake_image_data.detach())\n",
    "                d_error_fake = criterion(d_output_fake, labels_fake)\n",
    "                d_error_fake.backward()\n",
    "                \n",
    "                d_optim.step()\n",
    "                \n",
    "                # Akumulacija gubitka samo za logiranje\n",
    "                d_loss_accum += (d_err_real.item() + d_error_fake.item()) / d_updates\n",
    "\n",
    "\n",
    "            # ------------------------------------\n",
    "            #  2. Train Generator (G_updates puta)\n",
    "            # ------------------------------------\n",
    "            for _ in range(g_updates):\n",
    "                gmodel.zero_grad()\n",
    "                \n",
    "                # Generiranje novog noise vektora\n",
    "                noise = torch.randn(current_batch_size, 100, 1, 1, device=device)\n",
    "                fake_image_data = gmodel(noise)\n",
    "\n",
    "                # Želimo da D misli da su slike PRAVE (label=1)\n",
    "                labels_g = torch.ones(current_batch_size, device=device).float()\n",
    "                d_output_g = dmodel(fake_image_data) \n",
    "                \n",
    "                g_error = criterion(d_output_g, labels_g)\n",
    "                g_error.backward()\n",
    "                \n",
    "                g_optim.step()\n",
    "                \n",
    "                # Akumulacija gubitka\n",
    "                g_loss_accum += g_error.item() / g_updates\n",
    "            \n",
    "        # Normalizacija gubitka po broju batcheva\n",
    "        d_loss_avg = d_loss_accum / len(train_loader)\n",
    "        g_loss_avg = g_loss_accum / len(train_loader)\n",
    "            \n",
    "        if (epoch_idx + 1) % log_epochs == 0:\n",
    "            print(f\"[{epoch_idx+1}/{n_epochs} - D:{d_updates}x / G:{g_updates}x]: d_loss = {d_loss_avg:.5f} g_loss = {g_loss_avg:.5f}\")\n",
    "            \n",
    "    gmodel.eval()\n",
    "    dmodel.eval()\n",
    "    \n",
    "    return gmodel, dmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Treniranje: D:1x, G:1x (default)---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [23:24<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20 - D:1x / G:1x]: d_loss = 0.68125 g_loss = 4.51061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [22:56<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/20 - D:1x / G:1x]: d_loss = 0.32717 g_loss = 5.52453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:33<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/20 - D:1x / G:1x]: d_loss = 0.25065 g_loss = 6.55376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:28<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/20 - D:1x / G:1x]: d_loss = 0.28614 g_loss = 7.36969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:34<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/20 - D:1x / G:1x]: d_loss = 0.09342 g_loss = 7.17997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:28<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/20 - D:1x / G:1x]: d_loss = 0.00130 g_loss = 8.25808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:26<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/20 - D:1x / G:1x]: d_loss = 0.00044 g_loss = 8.76750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:37<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/20 - D:1x / G:1x]: d_loss = 0.00041 g_loss = 9.59166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:08<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/20 - D:1x / G:1x]: d_loss = 0.00018 g_loss = 10.78922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:32<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/20 - D:1x / G:1x]: d_loss = 0.00013 g_loss = 12.68565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:39<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/20 - D:1x / G:1x]: d_loss = 0.00005 g_loss = 12.36009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:14<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/20 - D:1x / G:1x]: d_loss = 0.00003 g_loss = 12.80252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:12<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/20 - D:1x / G:1x]: d_loss = 0.00003 g_loss = 13.12925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:15<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/20 - D:1x / G:1x]: d_loss = 0.00002 g_loss = 14.55634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:28<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/20 - D:1x / G:1x]: d_loss = 0.00002 g_loss = 14.10480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:21<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/20 - D:1x / G:1x]: d_loss = 0.00001 g_loss = 14.64081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:25<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/20 - D:1x / G:1x]: d_loss = 0.00001 g_loss = 14.81349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:26<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/20 - D:1x / G:1x]: d_loss = 0.00000 g_loss = 16.47643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [20:26<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/20 - D:1x / G:1x]: d_loss = 0.00000 g_loss = 16.00640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 230/469 [09:58<10:22,  2.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m dmodel_d1g1 \u001b[38;5;241m=\u001b[39m Discriminator()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Treniranje: D:1x, G:1x (default)---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m gmodel_d1g1, dmodel_d1g1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgmodel_d1g1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdmodel_d1g1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m random_sample_d1g1 \u001b[38;5;241m=\u001b[39m gmodel_d1g1(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[1;32mIn[15], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(gmodel, dmodel, n_epochs, log_epochs, batch_size, learning_rate, device, d_updates, g_updates)\u001b[0m\n\u001b[0;32m     43\u001b[0m labels_fake \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(current_batch_size, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# .detach() kako ne bismo računali gradijente za generator u ovom koraku\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m d_output_fake \u001b[38;5;241m=\u001b[39m \u001b[43mdmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_image_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m d_error_fake \u001b[38;5;241m=\u001b[39m criterion(d_output_fake, labels_fake)\n\u001b[0;32m     48\u001b[0m d_error_fake\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Ivor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ivor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[13], line 38\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu(x)\n\u001b[1;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(x)\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\Ivor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ivor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ivor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ivor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gmodel_d1g1 = Generator()\n",
    "dmodel_d1g1 = Discriminator()\n",
    "\n",
    "print(\"--- Treniranje: D:1x, G:1x (default)---\")\n",
    "gmodel_d1g1, dmodel_d1g1 = train(gmodel_d1g1, dmodel_d1g1, n_epochs=20, batch_size=128, device='cpu', d_updates=1, g_updates=1)\n",
    "\n",
    "random_sample_d1g1 = gmodel_d1g1(torch.randn(100, 100, 1, 1).to('cpu')).view(100, 64, 64).data.cpu().numpy()\n",
    "plt.figure(figsize=(10, 10))\n",
    "for idx in range(100):\n",
    "    plt.subplot(10, 10, idx+1)\n",
    "    plt.imshow(random_sample_d1g1[idx, ...], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Rezultati: Diskriminator 1x, Generator 1x', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('zad_gan.th', 'wb') as f:\n",
    "    torch.save(gmodel_d1g1.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_d2g1 = Generator()\n",
    "dmodel_d2g1 = Discriminator()\n",
    "\n",
    "# Ovo je scenarij u kojem D ima više prilika za učenje\n",
    "print(\"--- Treniranje: D:2x, G:1x ---\")\n",
    "gmodel_d2g1, dmodel_d2g1 = train(gmodel_d2g1, dmodel_d2g1, n_epochs=20, batch_size=128, device='cpu', d_updates=2, g_updates=1)\n",
    "\n",
    "random_sample_d2g1 = gmodel_d2g1(torch.randn(100, 100, 1, 1).to('cpu')).view(100, 64, 64).data.cpu().numpy()\n",
    "plt.figure(figsize=(10, 10))\n",
    "for idx in range(100):\n",
    "    plt.subplot(10, 10, idx+1)\n",
    "    plt.imshow(random_sample_d2g1[idx, ...], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Rezultati: Diskriminator 2x, Generator 1x', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_d1g2 = Generator()\n",
    "dmodel_d1g2 = Discriminator()\n",
    "\n",
    "# Ovo je scenarij u kojem G ima više prilika za učenje (očekuje se mode collapse)\n",
    "print(\"--- Treniranje: D:1x, G:2x ---\")\n",
    "gmodel_d1g2, dmodel_d1g2 = train(gmodel_d1g2, dmodel_d1g2, n_epochs=20, batch_size=128, device='cpu', d_updates=1, g_updates=2)\n",
    "\n",
    "random_sample_d1g2 = gmodel_d1g2(torch.randn(100, 100, 1, 1).to('cpu')).view(100, 64, 64).data.cpu().numpy()\n",
    "plt.figure(figsize=(10, 10))\n",
    "for idx in range(100):\n",
    "    plt.subplot(10, 10, idx+1)\n",
    "    plt.imshow(random_sample_d1g2[idx, ...], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Rezultati: Diskriminator 1x, Generator 2x', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_nobn = GeneratorNoBN()\n",
    "dmodel_nobn = DiscriminatorNoBN()\n",
    "\n",
    "print(\"--- Treniranje: BEZ Batch Normalizacije ---\")\n",
    "# Omjer 1:1\n",
    "gmodel_nobn, dmodel_nobn = train(gmodel_nobn, dmodel_nobn, n_epochs=20, batch_size=128, device='cpu', d_updates=1, g_updates=1)\n",
    "\n",
    "random_sample_nobn = gmodel_nobn(torch.randn(100, 100, 1, 1).to('cpu')).view(100, 64, 64).data.cpu().numpy()\n",
    "plt.figure(figsize=(10, 10))\n",
    "for idx in range(100):\n",
    "    plt.subplot(10, 10, idx+1)\n",
    "    plt.imshow(random_sample_nobn[idx, ...], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Rezultati: BEZ Batch Normalizacije', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
